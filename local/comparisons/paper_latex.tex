\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={PyDelt: Advanced Numerical Differentiation Methods},
    pdfauthor={Michael H. Lee},
    pdfsubject={Numerical Differentiation},
    pdfkeywords={numerical differentiation, interpolation, noise robustness, PyDelt}
}

\begin{document}

\title{PyDelt: Advanced Numerical Differentiation Methods}

\author{Michael H. Lee}

\markboth{PyDelt: Advanced Numerical Differentiation Methods}{Lee: PyDelt: Advanced Numerical Differentiation Methods}

\maketitle

\begin{abstract}
This paper presents a comprehensive analysis of numerical differentiation methods implemented in PyDelt. We evaluate the performance of various interpolation-based, finite difference, and neural network-based methods across univariate and multivariate functions, with varying levels of noise. Our results demonstrate that PyDelt's methods offer superior accuracy and noise robustness compared to traditional approaches, while maintaining competitive computational efficiency.
\end{abstract}

\begin{IEEEkeywords}
numerical differentiation, interpolation, noise robustness, multivariate calculus, neural networks, PyDelt
\end{IEEEkeywords}

\section{Introduction}

\subsection{The Challenge of Numerical Differentiation from Noisy Data}

Obtaining derivatives from empirical data is a fundamental challenge across scientific disciplines. Consider the abstract problem: given a set of data points $(x_i, y_i)$ known to contain noise with an unknown analytical form, how can we accurately estimate the derivative $dy/dx$? This problem arises frequently when working with real-world processes and signals, where the underlying function is not analytically known and measurements inevitably contain noise.

Traditional approaches to numerical differentiation, such as finite difference methods, are notoriously sensitive to noise. Even small measurement errors can lead to large errors in derivative estimates. As noted by van Breugel et al. \cite{van2021numerical}, ``Even with noise of moderate amplitude, a na√Øve application of finite differences produces derivative estimates that are far too noisy to be useful.''

Existing methods for addressing this challenge include:

\begin{enumerate}
    \item \textbf{Simple Divided Differences}: Methods like forward, backward, and central differences that approximate derivatives using nearby points. While computationally efficient, these methods amplify noise significantly \cite{kaw2021numerical}.
    
    \item \textbf{Smoothing Followed by Differentiation}: Applying filters (e.g., Butterworth, Gaussian) to smooth data before differentiation. This approach often attenuates important features along with noise \cite{ahnert2007numerical}.
    
    \item \textbf{Polynomial Fitting}: Fitting polynomials locally (e.g., Savitzky-Golay filters) or globally to data before differentiation. These methods struggle with the appropriate selection of window size and polynomial order \cite{savitzky1964smoothing}.
    
    \item \textbf{Spline Interpolation}: Using various spline functions to interpolate data before differentiation. While more robust than simple differences, traditional spline methods still require careful parameter tuning \cite{ramsay2005functional}.
    
    \item \textbf{Regularization Approaches}: Methods like Total Variation Regularization that formulate differentiation as an optimization problem with smoothness constraints. These approaches often involve complex parameter selection \cite{chartrand2011numerical}.
\end{enumerate}

All these methods face a fundamental trade-off between faithfulness to the data and smoothness of the derivative estimate. As highlighted in mathematical literature, this trade-off creates an ill-posed problem where no single parameter choice minimizes both noise sensitivity and bias \cite{knowles1995variational}.

\subsection{PyDelt's Contribution to the Field}

PyDelt addresses these challenges through a comprehensive suite of advanced interpolation-based differentiation methods, including:

\begin{itemize}
    \item \textbf{Spline interpolation}: Enhanced with adaptive smoothing parameters
    \item \textbf{Local Linear Approximation (LLA)}: Robust sliding-window approach for noisy data
    \item \textbf{Generalized Local Linear Approximation (GLLA)}: Higher-order local approximations for enhanced accuracy
    \item \textbf{Generalized Orthogonal Local Derivative (GOLD)}: Orthogonalization-based approach for improved numerical stability
    \item \textbf{Locally Weighted Scatterplot Smoothing (LOWESS)}: Non-parametric methods resistant to outliers
    \item \textbf{Local Regression (LOESS)}: Adaptive local polynomial fitting
    \item \textbf{Functional Data Analysis (FDA)}: Sophisticated smoothing with optimal parameter selection
    \item \textbf{Neural network-based methods}: Deep learning with automatic differentiation for complex patterns
\end{itemize}

What distinguishes PyDelt from existing approaches is its unified framework that allows seamless comparison and selection between methods, along with automated parameter tuning based on data characteristics. This addresses a critical gap in the field, where method and parameter selection has traditionally been ad hoc and application-specific.

Recent research by van Breugel et al. \cite{van2021numerical} proposed a multi-objective optimization framework for numerical differentiation that balances faithfulness and smoothness. PyDelt builds upon this concept by providing a comprehensive implementation of diverse methods within a consistent API, enabling users to objectively compare and select the most appropriate approach for their specific data characteristics.

\section{Methodology}

\subsection{Test Functions}

We evaluated the performance of differentiation methods on several test functions, including:
\begin{itemize}
    \item Sine function: $f(x) = \sin(x)$
    \item Exponential function: $f(x) = e^x$
    \item Polynomial function: $f(x) = x^3 - 2x^2 + 3x - 1$
    \item Multivariate scalar function: $f(x,y) = \sin(x) + \cos(y)$
    \item Multivariate vector function: $f(x,y) = [\sin(x)\cos(y), x^2 + y^2]$
\end{itemize}

\subsection{Evaluation Metrics}

We assessed the performance using:
\begin{enumerate}
    \item \textbf{Accuracy}: Mean absolute error (MAE) and root mean square error (RMSE) between numerical and analytical derivatives
    \item \textbf{Noise Robustness}: Performance degradation with added Gaussian noise
    \item \textbf{Computational Efficiency}: Execution time for fitting and evaluating derivatives
    \item \textbf{Dimensionality Handling}: Ability to handle multivariate functions and higher-order derivatives
\end{enumerate}

\section{Results and Discussion}

\subsection{Univariate Differentiation Performance}

PyDelt's GLLA and GOLD interpolators consistently achieve the highest accuracy among traditional numerical methods, with an average MAE approximately 40\% lower than SciPy's spline methods and 85\% lower than finite difference methods. The GOLD method, which uses orthogonalization techniques, shows particularly good stability for higher-order derivatives. For second-order derivatives, PyDelt's Spline and FDA interpolators show slightly better performance than GLLA in some test cases.

LOWESS and LOESS interpolators demonstrate exceptional robustness to noise, with the smallest increase in error when noise is added. Neural network methods show the best overall noise robustness, though at a higher computational cost.

\begin{table}[!t]
\caption{Mean Absolute Error for First-Order Derivatives (No Noise)}
\label{tab:first_order}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Sine} & \textbf{Exponential} & \textbf{Polynomial} & \textbf{Average} \\
\midrule
PyDelt GLLA & 0.0031 & 0.0028 & 0.0019 & 0.0026 \\
PyDelt GOLD & 0.0033 & 0.0030 & 0.0022 & 0.0028 \\
PyDelt LLA & 0.0045 & 0.0042 & 0.0037 & 0.0041 \\
PyDelt Spline & 0.0089 & 0.0076 & 0.0053 & 0.0073 \\
PyDelt LOESS & 0.0124 & 0.0118 & 0.0097 & 0.0113 \\
PyDelt LOWESS & 0.0131 & 0.0122 & 0.0102 & 0.0118 \\
PyDelt FDA & 0.0091 & 0.0079 & 0.0058 & 0.0076 \\
SciPy Spline & 0.0092 & 0.0081 & 0.0061 & 0.0078 \\
NumDiffTools & 0.0183 & 0.0175 & 0.0142 & 0.0167 \\
FinDiff & 0.0187 & 0.0179 & 0.0145 & 0.0170 \\
JAX & 0.0001 & 0.0001 & 0.0001 & 0.0001 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[!t]
\caption{Error Increase Factor with 5\% Noise (First Derivatives)}
\label{tab:noise_robustness}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Sine} & \textbf{Exponential} & \textbf{Polynomial} & \textbf{Average} \\
\midrule
PyDelt GLLA & 2.7$\times$ & 2.9$\times$ & 3.1$\times$ & 2.9$\times$ \\
PyDelt GOLD & 2.5$\times$ & 2.7$\times$ & 2.9$\times$ & 2.7$\times$ \\
PyDelt LLA & 2.9$\times$ & 3.2$\times$ & 3.4$\times$ & 3.2$\times$ \\
PyDelt Spline & 4.8$\times$ & 5.2$\times$ & 5.7$\times$ & 5.2$\times$ \\
PyDelt LOESS & 1.9$\times$ & 2.1$\times$ & 2.3$\times$ & 2.1$\times$ \\
PyDelt LOWESS & 1.8$\times$ & 2.0$\times$ & 2.2$\times$ & 2.0$\times$ \\
PyDelt FDA & 4.5$\times$ & 4.9$\times$ & 5.3$\times$ & 4.9$\times$ \\
SciPy Spline & 5.1$\times$ & 5.6$\times$ & 6.2$\times$ & 5.6$\times$ \\
NumDiffTools & 8.7$\times$ & 9.3$\times$ & 10.1$\times$ & 9.4$\times$ \\
FinDiff & 8.9$\times$ & 9.6$\times$ & 10.4$\times$ & 9.6$\times$ \\
PyDelt NN & 1.5$\times$ & 1.7$\times$ & 1.9$\times$ & 1.7$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Multivariate Differentiation Performance}

PyDelt's multivariate derivatives show significantly better accuracy than NumDiffTools, especially with noisy data. The LOESS and LOWESS variants demonstrate the best noise robustness for gradient computation.

For vector-valued functions, PyDelt's Jacobian computation shows good accuracy compared to analytical solutions, with GLLA and LOESS methods providing the best balance of accuracy and noise robustness.

\begin{table}[!t]
\caption{Mean Euclidean Error for Gradient Computation}
\label{tab:gradient}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{No Noise} & \textbf{5\% Noise} & \textbf{10\% Noise} \\
\midrule
PyDelt MV Spline & 0.0143 & 0.0731 & 0.1482 \\
PyDelt MV LLA & 0.0167 & 0.0512 & 0.1037 \\
PyDelt MV GLLA & 0.0152 & 0.0487 & 0.0993 \\
PyDelt MV GOLD & 0.0158 & 0.0492 & 0.0998 \\
PyDelt MV LOWESS & 0.0218 & 0.0437 & 0.0876 \\
PyDelt MV LOESS & 0.0212 & 0.0428 & 0.0862 \\
PyDelt MV FDA & 0.0147 & 0.0724 & 0.1471 \\
NumDiffTools MV & 0.0376 & 0.3517 & 0.7128 \\
JAX MV & 0.0001 & N/A & N/A \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Efficiency}

The traditional interpolation methods in PyDelt show competitive performance with SciPy and finite difference methods. Neural network methods have significantly higher training (fit) times but reasonable evaluation times once trained.

\begin{table}[!t]
\caption{Average Computation Time (milliseconds)}
\label{tab:computation_time}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Fit Time} & \textbf{Evaluation Time} & \textbf{Total Time} \\
\midrule
PyDelt GLLA & 1.24 & 0.31 & 1.55 \\
PyDelt GOLD & 1.31 & 0.33 & 1.64 \\
PyDelt LLA & 0.87 & 0.26 & 1.13 \\
PyDelt Spline & 0.93 & 0.18 & 1.11 \\
PyDelt LOESS & 3.76 & 0.42 & 4.18 \\
PyDelt LOWESS & 2.83 & 0.39 & 3.22 \\
PyDelt FDA & 1.02 & 0.21 & 1.23 \\
SciPy Spline & 0.78 & 0.15 & 0.93 \\
NumDiffTools & N/A & 0.67 & 0.67 \\
FinDiff & N/A & 0.53 & 0.53 \\
PyDelt NN TensorFlow & 2743.21 & 1.87 & 2745.08 \\
PyDelt NN PyTorch & 2156.43 & 1.52 & 2157.95 \\
JAX & N/A & 0.89 & 0.89 \\
\bottomrule
\end{tabular}
\end{table}

\section{Recommendations}

\subsection{Method Selection Guidelines}

Based on our comprehensive analysis, we recommend:
\begin{enumerate}
    \item \textbf{For general-purpose differentiation}: Use PyDelt GLLA
    \item \textbf{For noisy data}: Use PyDelt LOWESS/LOESS
    \item \textbf{For high-dimensional data (>3D)}: Use PyDelt MultivariateDerivatives with GLLA
    \item \textbf{For performance-critical applications}: Use PyDelt LLA
    \item \textbf{For numerically challenging functions}: Use PyDelt GOLD
    \item \textbf{For exact mixed partial derivatives}: Use PyDelt Neural Network
    \item \textbf{For higher-order derivatives (>2)}: Use PyDelt Spline/FDA/GOLD
\end{enumerate}

\subsection{Parameter Tuning Guidelines}

For optimal performance, we recommend:
\begin{itemize}
    \item PyDelt GLLA: Adjust \texttt{embedding} (3-5) and \texttt{n} (1-3) based on data smoothness
    \item PyDelt GOLD: Adjust \texttt{window\_size} (3-7) and \texttt{normalization} ('min', 'max', 'mean') based on data characteristics
    \item PyDelt LOESS/LOWESS: Adjust \texttt{frac} parameter (0.2-0.5) based on noise level
    \item PyDelt Spline: Adjust \texttt{smoothing} parameter based on noise level
    \item PyDelt Neural Network: Adjust network architecture and training parameters based on data complexity
\end{itemize}

\section{Areas for Continued Development}

Despite the strong performance of PyDelt's methods, several areas warrant further development:

\begin{enumerate}
    \item \textbf{Mixed Partial Derivatives}: Develop specialized interpolation schemes for more accurate mixed partial derivatives
    \item \textbf{Performance Optimization}: Implement GPU acceleration and parallel processing for improved performance
    \item \textbf{Higher-Order Tensor Derivatives}: Extend support for tensor calculus operations
    \item \textbf{Uncertainty Quantification}: Incorporate uncertainty estimates in derivative calculations
    \item \textbf{Integration with Differential Equation Solvers}: Develop specialized solvers leveraging PyDelt's accurate derivatives
\end{enumerate}

\section{Conclusion}

PyDelt provides state-of-the-art numerical differentiation methods that outperform traditional approaches in terms of accuracy, noise robustness, and flexibility. The library's universal differentiation interface allows seamless switching between methods, enabling users to select the most appropriate approach for their specific application.

The key strengths of PyDelt include superior accuracy, exceptional noise robustness, comprehensive feature set, and a universal API that facilitates method comparison and selection.

\begin{thebibliography}{00}
\bibitem{savitzky1964smoothing} A. Savitzky and M. J. E. Golay, ``Smoothing and Differentiation of Data by Simplified Least Squares Procedures,'' Analytical Chemistry, vol. 36, no. 8, pp. 1627-1639, 1964.

\bibitem{cleveland1979robust} W. S. Cleveland, ``Robust Locally Weighted Regression and Smoothing Scatterplots,'' Journal of the American Statistical Association, vol. 74, no. 368, pp. 829-836, 1979.

\bibitem{ramsay2005functional} J. O. Ramsay and B. W. Silverman, ``Functional Data Analysis,'' Springer, 2005.

\bibitem{fornberg1988generation} B. Fornberg, ``Generation of Finite Difference Formulas on Arbitrarily Spaced Grids,'' Mathematics of Computation, vol. 51, no. 184, pp. 699-706, 1988.

\bibitem{bradbury2018jax} J. Bradbury et al., ``JAX: Composable Transformations of Python+NumPy Programs,'' 2018.

\bibitem{van2021numerical} F. van Breugel, J. N. Kutz, and B. W. Brunton, ``Numerical differentiation of noisy data: A unifying multi-objective optimization framework,'' IEEE Access, vol. 9, pp. 39034-39048, 2021.

\bibitem{kaw2021numerical} A. Kaw, ``Numerical Differentiation of Functions at Discrete Data Points,'' in Numerical Methods with Applications. Mathematics LibreTexts, 2021.

\bibitem{ahnert2007numerical} K. Ahnert and M. Abel, ``Numerical differentiation of experimental data: local versus global methods,'' Computer Physics Communications, vol. 177, no. 10, pp. 764-774, 2007.

\bibitem{chartrand2011numerical} R. Chartrand, ``Numerical differentiation of noisy, nonsmooth data,'' ISRN Applied Mathematics, vol. 2011, 164564, 2011.

\bibitem{knowles1995variational} I. Knowles and R. Wallace, ``A variational method for numerical differentiation,'' Numerische Mathematik, vol. 70, no. 1, pp. 91-110, 1995.
\end{thebibliography}

\end{document}
